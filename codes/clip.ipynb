{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "187e9370-9be0-41de-b20d-660539575eae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch: 2.4.1+cu124\n",
      "torchvision: 0.19.1+cu124\n",
      "CUDA available?: True\n",
      "GPU: NVIDIA A40\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "\n",
    "print(\"torch:\", torch.__version__)\n",
    "print(\"torchvision:\", torchvision.__version__)\n",
    "print(\"CUDA available?:\", torch.cuda.is_available())\n",
    "print(\"GPU:\", torch.cuda.get_device_name(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e6c93e80-995d-40a7-b7f8-4f16d204be4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable.It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# ⚙️ 1. 패키지 설치 (필요 시)\n",
    "# transformers, pillow 설치가 아직 안 되어 있으면 아래 주석을 해제하고 실행하세요.\n",
    "!pip install transformers>=4.40.0 pillow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "809605a2-8fb7-4989-a505-d514ac7a502e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. 라이브러리 임포트\n",
    "import torch\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "from transformers import CLIPProcessor, CLIPModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "02286fdb-0ff9-4634-88d0-dc55c8e84f28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n"
     ]
    }
   ],
   "source": [
    "# 3. 디바이스 설정\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "59b6745d-cc5f-41be-ac8c-6e8b839e6bc9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e41630638f24e1a8e3db2687b1f299f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "preprocessor_config.json:   0%|          | 0.00/316 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e9643a0c1d2047b4a5d9e71ee5cd9158",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/592 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b08efc78fe9149c8b4136fd4429dddac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/862k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "594000aee7484dbdbdffcb03e63a9f3f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/525k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "46622016eaf44d73a74f9c2a5e7f5e4a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/2.22M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef35da65f60748e5b9f751682fb84ad6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/389 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cc8e5846857c4a5f9953fa9389e4b47c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/4.19k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "96300eee15164254b519c834d2035b8c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/605M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 6. 텍스트 전처리\n",
    "# 비교할 텍스트 문장들을 리스트에 담습니다.\n",
    "from transformers import CLIPTokenizerFast, CLIPImageProcessor\n",
    "\n",
    "model_name = \"openai/clip-vit-base-patch32\"\n",
    "\n",
    "# 1) 이미지 전처기: Fast 버전 사용\n",
    "image_processor = CLIPImageProcessor.from_pretrained(model_name, use_fast=True)\n",
    "\n",
    "# 2) 토크나이저: Fast 토크나이저 사용\n",
    "tokenizer = CLIPTokenizerFast.from_pretrained(model_name)\n",
    "\n",
    "# Safetensors 포맷으로 가중치를 불러오도록 설정\n",
    "# torch 버전이 2.6 미만이라도 안전하게 로드됩니다.\n",
    "model = CLIPModel.from_pretrained(\n",
    "    model_name,\n",
    "    use_safetensors=True    # ✨ 이 옵션을 추가하세요\n",
    ").to(device)\n",
    "\n",
    "# 6. 텍스트 전처리 (수정)\n",
    "texts = [\n",
    "    \"a photo of a cat\",\n",
    "    \"a photo of a dog\",\n",
    "    \"a landscape with mountains\"\n",
    "]\n",
    "# Fast 토크나이저를 이용해 토큰화 + 패딩\n",
    "text_inputs = tokenizer(\n",
    "    texts,\n",
    "    return_tensors=\"pt\",\n",
    "    padding=True,      # 길이 맞춤\n",
    "    truncation=True    # 필요 시 자르기\n",
    ")\n",
    "input_ids = text_inputs.input_ids.to(device)\n",
    "attention_mask = text_inputs.attention_mask.to(device)\n",
    "\n",
    "# 5. 이미지 전처리 (별도 적용)\n",
    "image_path = \"Golden-Retriever.jpg\"\n",
    "image = Image.open(image_path).convert(\"RGB\")\n",
    "image_inputs = image_processor(\n",
    "    images=image,\n",
    "    return_tensors=\"pt\"\n",
    ")\n",
    "pixel_values = image_inputs.pixel_values.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a9caee6a-92f0-49dd-bc88-2fc79aa452eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7. 임베딩 생성\n",
    "with torch.no_grad():\n",
    "    # 이미지 임베딩 (batch_size=1, 512차원)\n",
    "    image_embeds = model.get_image_features(pixel_values=pixel_values)\n",
    "    # 텍스트 임베딩 (batch_size=len(texts), 512차원)\n",
    "    text_embeds = model.get_text_features(\n",
    "        input_ids=input_ids,\n",
    "        attention_mask=attention_mask\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6caeb9d9-f923-49f5-b05c-0e6ab614b4f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8. 유사도 계산 (코사인 유사도)\n",
    "# 먼저 임베딩을 정규화(normalize)합니다.\n",
    "image_embeds = image_embeds / image_embeds.norm(p=2, dim=-1, keepdim=True)\n",
    "text_embeds = text_embeds / text_embeds.norm(p=2, dim=-1, keepdim=True)\n",
    "\n",
    "# 코사인 유사도: image_embeds @ text_embeds.T\n",
    "similarity = (image_embeds @ text_embeds.T).squeeze(0)  # shape: (len(texts),)\n",
    "# CPU로 옮겨서 출력\n",
    "similarity = similarity.cpu().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d3734455-c3e9-40cc-8dfc-a07286b02422",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‘a photo of a cat’ 유사도: 0.1978\n",
      "‘a photo of a dog’ 유사도: 0.2683\n",
      "‘a landscape with mountains’ 유사도: 0.1775\n"
     ]
    }
   ],
   "source": [
    "# 9. 결과 출력\n",
    "for text, score in zip(texts, similarity):\n",
    "    print(f\"‘{text}’ 유사도: {score:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "233f3d08-e221-4152-aa0d-c3810c7be3eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Zero-shot 분류 결과: 'a photo of a dog' (확률 0.999)\n"
     ]
    }
   ],
   "source": [
    "# 10. Zero-shot 분류 예제\n",
    "# CLIPModel의 로지트(로그잇) 스코어를 사용하여 가장 유사한 문장 선택\n",
    "with torch.no_grad():\n",
    "    outputs = model(\n",
    "        pixel_values=pixel_values,\n",
    "        input_ids=input_ids,\n",
    "        attention_mask=attention_mask\n",
    "    )\n",
    "    logits_per_image = outputs.logits_per_image.squeeze(0)  # 이미지 → 텍스트 스코어\n",
    "    probs = logits_per_image.softmax(dim=-1).cpu().tolist()\n",
    "\n",
    "# 확률 기반으로 가장 높은 라벨 확인\n",
    "best_idx = int(torch.argmax(logits_per_image))\n",
    "print(f\"\\nZero-shot 분류 결과: '{texts[best_idx]}' (확률 {probs[best_idx]:.3f})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6178430d-c910-4d0a-9fa6-3f2fa1584cc3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
